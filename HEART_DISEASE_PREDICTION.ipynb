{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "16759e71e0db7e5458cd37a19fbf7b21c24e7301",
        "id": "cAhPDv5IApH6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Loading and preparing the dataset\n",
        "dataset = pd.read_csv(\"/content/heart.csv\")\n",
        "X = dataset.drop(\"target\", axis=1).values\n",
        "y = dataset[\"target\"].values\n",
        "\n",
        "# Standardizing the feature set for Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "### Logistic Regression Implementation from Scratch\n",
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.05, n_iterations=5000):  # Adjusted learning rate and iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "\n",
        "# Random Forest and Decision Tree Implementations from Scratch\n",
        "class DecisionTreeScratch:\n",
        "    def __init__(self, max_depth=10):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if depth >= self.max_depth or n_labels == 1 or n_samples < 2:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        left_indices = X[:, best_feat] < best_thresh\n",
        "        right_indices = X[:, best_feat] >= best_thresh\n",
        "        left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return (best_feat, best_thresh, left, right)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gini = 1.0\n",
        "        split_idx, split_thresh = None, None\n",
        "        for i in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, i])\n",
        "            for t in thresholds:\n",
        "                left = y[X[:, i] < t]\n",
        "                right = y[X[:, i] >= t]\n",
        "                gini = self._gini_impurity(left, right)\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    split_idx, split_thresh = i, t\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _gini_impurity(self, left, right):\n",
        "        def gini(y):\n",
        "            classes, counts = np.unique(y, return_counts=True)\n",
        "            return 1.0 - sum((count / len(y)) ** 2 for count in counts)\n",
        "\n",
        "        n = len(left) + len(right)\n",
        "        gini_left = gini(left)\n",
        "        gini_right = gini(right)\n",
        "        return (len(left) / n) * gini_left + (len(right) / n) * gini_right\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(inputs, self.tree) for inputs in X]\n",
        "\n",
        "    def _predict(self, inputs, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "        feature, threshold, left, right = node\n",
        "        if inputs[feature] < threshold:\n",
        "            return self._predict(inputs, left)\n",
        "        else:\n",
        "            return self._predict(inputs, right)\n",
        "\n",
        "class RandomForestScratch:\n",
        "    def __init__(self, n_trees=20, max_depth=10):  # Increased trees and depth\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_trees):\n",
        "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[idxs], y[idxs]\n",
        "            tree = DecisionTreeScratch(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return [Counter(tree_pred).most_common(1)[0][0] for tree_pred in tree_preds.T]\n",
        "\n",
        "# Training and Testing Logistic Regression\n",
        "lr_model = LogisticRegressionScratch(learning_rate=0.05, n_iterations=5000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Training and Testing Random Forest\n",
        "rf_model = RandomForestScratch(n_trees=20, max_depth=10)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculating and Displaying Metrics using sklearn's library functions\n",
        "print(\"Logistic Regression:\")\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions) * 100\n",
        "lr_precision = precision_score(y_test, lr_predictions) * 100\n",
        "lr_recall = recall_score(y_test, lr_predictions) * 100\n",
        "lr_f1 = f1_score(y_test, lr_predictions) * 100\n",
        "print(f\"Accuracy: {lr_accuracy:.2f}%\")\n",
        "print(f\"Precision: {lr_precision:.2f}%\")\n",
        "print(f\"Recall: {lr_recall:.2f}%\")\n",
        "print(f\"F1-Score: {lr_f1:.2f}%\\n\")\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions) * 100\n",
        "rf_precision = precision_score(y_test, rf_predictions) * 100\n",
        "rf_recall = recall_score(y_test, rf_predictions) * 100\n",
        "rf_f1 = f1_score(y_test, rf_predictions) * 100\n",
        "print(f\"Accuracy: {rf_accuracy:.2f}%\")\n",
        "print(f\"Precision: {rf_precision:.2f}%\")\n",
        "print(f\"Recall: {rf_recall:.2f}%\")\n",
        "print(f\"F1-Score: {rf_f1:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBIvyrQKsyO",
        "outputId": "f033cb32-d336-4806-9d33-4e0013086419"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Accuracy: 85.25%\n",
            "Precision: 87.10%\n",
            "Recall: 84.38%\n",
            "F1-Score: 85.71%\n",
            "\n",
            "Random Forest:\n",
            "Accuracy: 85.25%\n",
            "Precision: 87.10%\n",
            "Recall: 84.38%\n",
            "F1-Score: 85.71%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}